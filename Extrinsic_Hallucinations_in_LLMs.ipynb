{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTRt4xCha5FzC8LUKMYFNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/self-improvement/blob/main/Extrinsic_Hallucinations_in_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://lilianweng.github.io/posts/2024-07-07-hallucination/"
      ],
      "metadata": {
        "id": "MhCG7PZ2Y929"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Understanding Hallucinations in LLMs**\n",
        "\n",
        "**Definition:** Hallucination in LLMs refers to situations where models generate responses that are unfaithful to their training data, sometimes fabricating information, producing contradictions, or generating nonsense.\n",
        "\n",
        "**Focus on Extrinsic Hallucination:** The document focuses specifically on extrinsic hallucinations, where the output diverges from factual information or what would be expected based on world knowledge and the given context.\n",
        "\n",
        "# **2. Causes of Hallucinations**\n",
        "\n",
        "### **Pre-training Data Issues:**\n",
        "\n",
        "Since pre-training uses vast amounts of publicly available data, models can “learn” from outdated or incorrect information. As the training data aims to represent general world knowledge, the sheer volume and variability lead to potential inaccuracies.\n",
        "Issues arise when models try to maximize probability (log-likelihood) across such large datasets, often resulting in memorization of errors.\n",
        "\n",
        "### **Challenges in Fine-tuning:**\n",
        "\n",
        "Fine-tuning is essential to align models with more current or reliable information. However, it's challenging to ensure comprehensive coverage of all updated facts or newly emerging knowledge, leading to potential hallucinations if the model is asked about unlearned topics.\n",
        "\n",
        "# **3. Detection Techniques for Hallucinations**\n",
        "\n",
        "### **Retrieval-Augmented Evaluation:**\n",
        "\n",
        "A strategy where models are tested against trusted data sources or retrieval systems that provide grounded information, which the model's response can then be checked against.\n",
        "\n",
        "### **Sampling-Based Detection:**\n",
        "This involves generating multiple model responses to the same prompt and checking for consistency. If the responses differ significantly, it may indicate hallucination.\n",
        "\n",
        "# **4. Mitigation and Anti-Hallucination Methods**\n",
        "\n",
        "### **Calibration of Unknown Knowledge:**\n",
        "Models are calibrated to recognize when they do not know something. By incorporating mechanisms that encourage the model to “defer” or acknowledge a lack of information, hallucinations can be reduced.\n",
        "\n",
        "### **Indirect Querying:**\n",
        "This involves reformulating questions or asking for a sequence of related queries that guide the model to provide more grounded answers.\n",
        "Retrieval-Augmented Generation (RAG):\n",
        "RAG techniques use external data retrieval systems during generation to incorporate real-time or accurate information, aiming to ground responses more firmly in actual data.\n",
        "\n",
        "### **Fine-tuning for Factuality and Attribution:**\n",
        "Models are fine-tuned not just for response accuracy but also to attribute information correctly. This might involve using sources more reliably and providing justification for responses.\n",
        "\n",
        "# **5. Additional Methods for Reducing Hallucinations**\n",
        "\n",
        "### **Chain of Actions Sampling:**\n",
        "Here, the model goes through a sequence of logical steps before generating an answer, which can reduce the tendency to produce ungrounded content.\n",
        "\n",
        "### **Sampling Methods:**\n",
        "This includes techniques like top-k sampling or temperature adjustments, which make the model less likely to produce outlier responses by focusing on high-probability outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "lHyWNYnAZCAl"
      }
    }
  ]
}