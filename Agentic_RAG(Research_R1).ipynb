{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwsQp/7MShd5FBBTutnYWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/self-improvement/blob/main/Agentic_RAG(Research_R1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplified version of Agentic RAG using **Search-R1**\n",
        "\n",
        "> Policy gradient simulate GRPO\n"
      ],
      "metadata": {
        "id": "YBKI5vTcpn3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhNizNQMpf9S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.optim import Adam\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
        "\n",
        "class SearchEngine:\n",
        "    def search(self, query):\n",
        "        # search and return outcome\n",
        "        pass\n",
        "\n",
        "def generate_trajectory(model, tokenizer, question, search_engine):\n",
        "    trajectory = []\n",
        "    actions = []\n",
        "    log_probs = []\n",
        "    state = question\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # inference\n",
        "        inputs = tokenizer(state, return_tensors=\"pt\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100, output_scores=True, return_dict_in_generate=True)\n",
        "\n",
        "        # get output and prob\n",
        "        step_output = tokenizer.decode(outputs.sequences[0])\n",
        "        action_prob = torch.softmax(outputs.scores[-1], dim=-1).max().item()  # simplified\n",
        "\n",
        "        trajectory.append(step_output)\n",
        "\n",
        "        # check whether search again\n",
        "        if \"<search>\" in step_output:\n",
        "            # log action and prob\n",
        "            actions.append(\"search\")\n",
        "            log_probs.append(action_prob)\n",
        "\n",
        "            # extract query\n",
        "            query = extract_search_query(step_output)\n",
        "\n",
        "            # search\n",
        "            search_results = search_engine.search(query)\n",
        "\n",
        "            # update\n",
        "            state = state + step_output + search_results\n",
        "        else:\n",
        "            # log action and prob\n",
        "            actions.append(\"answer\")\n",
        "            log_probs.append(action_prob)\n",
        "\n",
        "            # generate answer\n",
        "            done = True\n",
        "\n",
        "    return trajectory, actions, torch.tensor(log_probs)\n",
        "\n",
        "# define RL\n",
        "def train_rl(model, dataset, search_engine, epochs=3, lr=1e-5):\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for question, answer in dataset:\n",
        "            # generate trajectory including search actions\n",
        "            with torch.no_grad():\n",
        "                trajectory, actions, log_probs = generate_trajectory(\n",
        "                    model, tokenizer, question, search_engine\n",
        "                )\n",
        "\n",
        "            # cal rewards\n",
        "            final_answer = trajectory[-1]\n",
        "            reward = compute_reward(final_answer, answer)\n",
        "\n",
        "            # cal gradient loss\n",
        "            policy_loss = -torch.mean(log_probs * reward)\n",
        "\n",
        "            # update model\n",
        "            optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "def extract_search_query(text):\n",
        "    # suppose format: <search>query</search>\n",
        "    start_tag = \"<search>\"\n",
        "    end_tag = \"</search>\"\n",
        "    start_idx = text.find(start_tag) + len(start_tag)\n",
        "    end_idx = text.find(end_tag)\n",
        "\n",
        "    if start_idx >= len(start_tag) and end_idx > start_idx:\n",
        "        return text[start_idx:end_idx].strip()\n",
        "    return \"\"\n",
        "\n",
        "def compute_reward(prediction, ground_truth):\n",
        "\n",
        "    if prediction == ground_truth:\n",
        "        return 1.0\n",
        "    return 0.0"
      ]
    }
  ]
}